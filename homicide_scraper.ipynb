{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.9.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\José Luis Adriano\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\José Luis Adriano\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sodapy\n",
      "  Downloading sodapy-2.2.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests>=2.28.1 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sodapy) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.28.1->sodapy) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\josé luis adriano\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.28.1->sodapy) (2.1.0)\n",
      "Installing collected packages: sodapy\n",
      "Successfully installed sodapy-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\José Luis Adriano\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import timedelta, date\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "%pip install lxml\n",
    "%pip install requests\n",
    "%pip install sodapy\n",
    "\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "from sodapy import Socrata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Version 1 Data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get limit (needs debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: ideally change to webpage with the data\n",
    "page = requests.get('https://www.dallasopendata.com/Public-Safety/Police-Incidents/qv6i-rri7')\n",
    "webpage = html.fromstring(page.content)\n",
    "\n",
    "#NOTE: this does not work right now\n",
    "lim = webpage.xpath('/html/body/main/div/div[1]/div/div[2]/forge-view-switcher/forge-view[1]/div/div/div/div[1]/section[1]/div/div/dl/div[1]/dd')\n",
    "print(lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "#Define Socrata API Client\n",
    "client = Socrata(\"www.dallasopendata.com\", None)\n",
    "\n",
    "#NOTE: Change to the limit we defined earlier\n",
    "lim = 1200000\n",
    "\n",
    "#Read in data and convert to DataFrame\n",
    "results = client.get(\"qv6i-rri7\", limit=lim)\n",
    "\n",
    "df = pd.DataFrame.from_records(results)\n",
    "\n",
    "    # lim += 100000\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#Create empty lists of addresses, latitudes and longitudes to convert to columns later.\n",
    "lats = []\n",
    "longs = []\n",
    "adds = []\n",
    "\n",
    "#In the underlying raw date we have a column called 'Location1' where each cell contains a street address, latitude and Longitude. Lets extract those and add them to the list.\n",
    "for i in range(0, len(df)):\n",
    "    #If the cell isn't empty\n",
    "    try:\n",
    "        latitude = df['geocoded_column'][i]['latitude']\n",
    "    except:\n",
    "            latitude = np.nan\n",
    "    try:\n",
    "        longitude = df['geocoded_column'][i]['longitude']\n",
    "    except:\n",
    "        longitude = np.nan\n",
    "    try:\n",
    "        address = ''\n",
    "        for val in ast.literal_eval(df['geocoded_column'][i]['human_address']).values():\n",
    "             address = address + ' ' + val.strip()\n",
    "    except:\n",
    "         address = ''\n",
    "        \n",
    "    #Add the lat, long and address to the corresponding list.\n",
    "    lats.append(latitude)\n",
    "    longs.append(longitude)\n",
    "    adds.append(address)\n",
    "\n",
    "# #Create new columns from the populated lists.\n",
    "df['latitude'] = lats\n",
    "df['longitude'] = longs\n",
    "df['address'] = adds \n",
    "\n",
    "df['date1'] = pd.to_datetime(df['date1'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Version 2 Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data are stored in Excel spreadsheets with one year's worth of incident data. Let's get a list of these files, so we can combine them. \n",
    "path = 'data/raw/dpd/'\n",
    "files = os.listdir(path)\n",
    "files = [i for i in files if 'Open_Data' in i]\n",
    "\n",
    "#Create an empty dataframe to add our data to.\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "#Go through each file.\n",
    "for file in files:\n",
    "    #Most of our files just use the default 'Sheet1' for their sheet name. But in 2018, 2019, 2020 and 2022 the sheet name is the year. \n",
    "    if file not in ['Open_Data_2018.xlsx', 'Open_Data_2019.xlsx', 'Open_Data_2020.xlsx', 'Open_Data_2022.xlsx']:\n",
    "        #If the data is not one of those years, then just use Sheet1 as the sheet name. \n",
    "        dfx = pd.read_excel(f'{path}{file}', sheet_name = 'Sheet1')\n",
    "\n",
    "        #Try to extract the year from the filename and assign that value to a column.\n",
    "        try:\n",
    "            dfx['year'] = int(file[-9:-5])\n",
    "        \n",
    "        #If the year isn't in that position on the string, it's our half year of data from 2014.\n",
    "        except:\n",
    "            dfx['year'] = 2014 \n",
    "    \n",
    "    #If it is one of those years, extract it from the filename, use that as the sheet name an assign that value to a column.\n",
    "    else:\n",
    "       dfx = pd.read_excel(f'{path}{file}', sheet_name = file[-9:-5])\n",
    "       dfx['year'] = int(file[-9:-5])\n",
    "    \n",
    "    #Add that data to our empty df\n",
    "    df1 = pd.concat([df1, dfx])\n",
    "\n",
    "#Each of the files we add to the empty df will start from 0, so we need to reset the index so that the numbers are unique for each row.\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1.columns = [i.lower().strip().replace(' ', '_').replace('/', '').replace('(', '').replace(')', '').replace('#', '').replace('-', '') for i in list(df1.columns)]\n",
    "\n",
    "df1['point_x'] = pd.to_numeric(df1['point_x'], errors='coerce')\n",
    "df1['point_y'] = pd.to_numeric(df1['point_y'], errors='coerce')\n",
    "\n",
    "#convert incidents to a spatial file\n",
    "gdf = gpd.GeoDataFrame(df1, geometry = gpd.points_from_xy(df1['point_x'], df1['point_y']))\n",
    "\n",
    "#project using DPDs weird CRS\n",
    "gdf = gdf.set_crs(\"EPSG:2276\")\n",
    "\n",
    "#re-project using a normal CRS\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "gdf['latitude'] = gdf['geometry'].y\n",
    "gdf['longitude'] = gdf['geometry'].x\n",
    "\n",
    "df1 = gdf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Version 3 Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in incidents data\n",
    "df2a = pd.read_excel('data/raw/dpd/Data From Jan 1, 2017 to Apr 13, 2023.xlsx', sheet_name='From Jan 1, 2017-Apr 13, 2023')\n",
    "df2b = pd.read_excel('data/raw/dpd/Data of 2014 to 2016.xls', sheet_name=' Murder Data 2014-2016 (01%)')\n",
    "df2c = pd.read_excel('data/raw/dpd/Data of 2014 to 2016.xls', sheet_name='(03%)  Robbery Data 2014-2016')\n",
    "df2d = pd.read_excel('data/raw/dpd/Data of 2014 to 2016.xls', sheet_name='(04%) Agg Assault 2014-2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df2a, df2b, df2c, df2d])\n",
    "df2.columns = [i.lower().strip().replace(' ', '_').replace('/', '').replace('(', '').replace(')', '').replace('#', '').replace('-', '') for i in list(df2.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['point_x'] = pd.to_numeric(df2['point_x'], errors='coerce')\n",
    "df2['point_y'] = pd.to_numeric(df2['point_y'], errors='coerce')\n",
    "\n",
    "#convert incidents to a spatial file\n",
    "gdf = gpd.GeoDataFrame(df2, geometry = gpd.points_from_xy(df2['point_x'], df2['point_y']))\n",
    "#project using DPDs weird CRS\n",
    "gdf = gdf.set_crs(\"EPSG:2276\")\n",
    "\n",
    "#re-project using a normal CRS\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "gdf['latitude'] = gdf['geometry'].y\n",
    "gdf['longitude'] = gdf['geometry'].x\n",
    "\n",
    "df2 = gdf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Version 4 Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in latest batch of incidents\n",
    "df3 = pd.read_excel('data/raw/dpd/D009766-032123__Data_from_2014-to_present.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase all of our columns then, strip whitespace, replace spaces with underscores and get rid of unnecssary symbols like slashes, dashes, parentheses and number signs.\n",
    "df3.columns = [i.lower().strip().replace(' ', '_').replace('/', '').replace('(', '').replace(')', '').replace('#', '').replace('-', '') for i in list(df3.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert incidents to a spatial file\n",
    "gdf = gpd.GeoDataFrame(df3, geometry = gpd.points_from_xy(df3['point_x'], df3['point_y']))\n",
    "\n",
    "#project using DPDs weird CRS\n",
    "gdf = gdf.set_crs(\"EPSG:2276\")\n",
    "\n",
    "#re-project using a normal CRS\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['latitude'] = gdf['geometry'].y\n",
    "gdf['longitude'] = gdf['geometry'].x\n",
    "df3 = gdf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Combining the Data<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['version'] = 1\n",
    "df1['version'] = 2\n",
    "df2['version'] = 3\n",
    "df3['version'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'zip_code': 'zipcode', 'x_coordinate': 'point_x', 'y_cordinate':'point_y' })\n",
    "print('not in df1: ', [i for i in list(df.columns) if i not in list(df1.columns)])\n",
    "print('not in df2: ', [i for i in list(df.columns) if i not in list(df2.columns)])\n",
    "print('not in df3: ', [i for i in list(df.columns) if i not in list(df3.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns = {'apt#':'apt'})\n",
    "print('not in df: ', [i for i in list(df1.columns) if i not in list(df.columns)])\n",
    "print('not in df2: ', [i for i in list(df1.columns) if i not in list(df2.columns)])\n",
    "print('not in df3: ', [i for i in list(df1.columns) if i not in list(df3.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns = {'apt#':'apt'})\n",
    "print('not in df: ', [i for i in list(df2.columns) if i not in list(df.columns)])\n",
    "print('not in df1: ', [i for i in list(df2.columns) if i not in list(df1.columns)])\n",
    "print('not in df3: ', [i for i in list(df2.columns) if i not in list(df3.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('not in df: ', [i for i in list(df3.columns) if i not in list(df.columns)])\n",
    "print('not in df1: ', [i for i in list(df3.columns) if i not in list(df1.columns)])\n",
    "print('not in df2: ', [i for i in list(df3.columns) if i not in list(df2.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Filter to Just Murders</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date1'] = pd.to_datetime(df['date1'], errors='coerce')\n",
    "\n",
    "df = (df\n",
    "      .loc[((df['nibrs_code'] == '09A') | \n",
    "            (df['ucrcode'].isin([110, 120])))]\n",
    "        .sort_values('date1', ascending=False)\n",
    "        .drop_duplicates(subset = ['incidentnum'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "        .copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/created/dpd_murders_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
